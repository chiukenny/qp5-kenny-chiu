\documentclass[10pt]{article}
\input{../report/header}
\input{../report/defs}


\begin{document}

\tableofcontents

\newpage

\section{Unbiased Implicit Variational Inference}

Based on \citet{Titsias:2019}.

\begin{itemize}

\item
Authors introduce unbiased implicit variational inference (UIVI) that defines a flexible variational family. Like semi-implicit variational inference (SIVI), UIVI uses an implicit variational distribution $q_\theta(z)=\int q_\theta(z|\varepsilon)q(\varepsilon)d\varepsilon$ where $q_\theta(z|\varepsilon)$ is a reparameterizable distribution whose parameters can be outputs of some neural network $g$, i.e., $q_\theta(z|\varepsilon)=h(u;g(\varepsilon;\theta))$ with $u\sim q(u)$. Under two assumptions on the conditional $q_\theta(z|\varepsilon)$, the ELBO can be approximated via Monte Carlo sampling. In particular, the entropy component of the ELBO can be rewritten as an expectation w.r.t. the reverse conditional $q_\theta(\varepsilon|z)$. Efficient approximation of this expectation w.r.t. the reverse conditional is done by reusing samples from approximating the main expectation to initialize a MCMC sampler.

\item
In \sivi, the variational distribution $q_\theta(z)$ is defined as
\[
q_\theta(z) = \int q_\theta(z|\eps)q(\eps)d\eps
\]
where $\eps\sim q(\eps)$.

\item
\uivi:
\begin{itemize}
\item
Like \sivi, \uivi uses an implicit variational distribution $q_\theta(z)$ whose density cannot be evaluated but from which samples can be drawn. Unlike \sivi, \uivi directly maximizes the \elbo rather than a lower bound.
\item
The dependence of $q_\theta(z|\eps)$ on $\eps$ can be arbitrarily complex. \citet{Titsias:2019} take the parameters of a reparameterizable distribution (Assumption~1) as the output of a neural network with parameters $\theta$ that takes $\eps$ as input, i.e.,
\[
z = h(u; g_\theta(\eps)) = h_\theta(u;\eps)
\]
where $u\sim q(u)$ and $g_\theta$ is some neural network. It is also assumed that $\nabla_z\log q_\theta(z|\eps)$ can be evaluated (Assumption~2).
\item
The gradient of the \elbo is given by
\begin{align*}
\nabla_\theta\calL(\theta) &= \nabla_\theta\E_{q_\theta(z)}\left[\log p(x,z) - \log q_\theta(z)\right] \\
&= \nabla_\theta\int\left(\log p(x,z) - \log q_\theta(z)\right)q_\theta(z)dz \\
&= \int\nabla_\theta\left(\left(\log p(x,z) - \log q_\theta(z)\right)q_\theta(z)\right)dz \\
&= \int\nabla_\theta\left(\left(\log p(x,z) - \log q_\theta(z)\right)\int q_\theta(z|\eps)q(\eps)d\eps\right)dz \\
&= \int\int\nabla_\theta\left(\left(\log p(x,z) - \log q_\theta(z)\right)\big|_{z=h_\theta(u;\eps)}\right)q(u)q(\eps)d\eps du \\
&= \E_{q(\eps)q(u)}\left[\nabla_z\log p(x,z)\big|_{z=h_\theta(u;\eps)}\nabla_\theta h_\theta(u;\eps)\right] - \E_{q(\eps)q(u)}\left[\nabla_z\log q_\theta(z)\big|_{z=h_\theta(u;\eps)}\nabla_\theta h_\theta(u;\eps)\right] \;.
\end{align*}
(Note that is $\E_{q_\theta(z)}\left[\nabla_\theta\log q_\theta(z)\right]=0$ is applied as below; see \href{https://bayesgroup.github.io/bmml_sem/2018/Molchanov_Implicit%20Models_2018_2.pdf}{Slide 24}) (Gradient can be pushed into expectation using DCT.)

\begin{align*}
\nabla_\theta\E_{q_\theta(z)}\left[\log q_\theta(z)\right] &= \nabla_\theta\E_{q(\eps)}\left[\log q_\theta(f_\theta(\eps))\right] \\
&= \E_{q(\eps)}\left[\nabla_\theta \log q_\theta(z)\big|_{z=f_\theta(\eps)}\right] + \E_{q(\eps)}\left[\nabla_z\log q_\theta(z)\big|_{z=f_\theta(\eps)}\nabla_\theta f_\theta(\eps)\right] \\
&= \E_{q_\theta(z)}\left[\nabla_\theta \log q_\theta(z)\right] + \E_{q(\eps)}\left[\nabla_z\log q_\theta(z)\big|_{z=f_\theta(\eps)}\nabla_\theta f_\theta(\eps)\right] \\
&= \E_{q(\eps)}\left[\nabla_z\log q_\theta(z)\big|_{z=f_\theta(\eps)}\nabla_\theta f_\theta(\eps)\right]
\end{align*}

As $\nabla_z\log q_\theta(z)$ cannot be evaluated, this gradient is rewritten as an expectation using the log-deritative identity: $\nabla_x\log f(x) = \frac{1}{f(x)}\nabla_x f(x)$:
\begin{align*}
\nabla_z\log q_\theta(z) &= \frac{1}{q_\theta(z)}\nabla_zq_\theta(z) \\
&= \frac{1}{q_\theta(z)}\nabla_z\int q_\theta(z|\eps)q(\eps)d\eps \\
&= \frac{1}{q_\theta(z)}\int \nabla_z q_\theta(z|\eps)q(\eps)d\eps \\
&= \frac{1}{q_\theta(z)}\int q_\theta(z|\eps)q(\eps)\nabla_z\log q_\theta(z|\eps)d\eps \\
&= \int q_\theta(\eps|z)\nabla_z\log q_\theta(z|\eps)d\eps \\
&= \E_{q_\theta(\eps|z)}\left[\nabla_z\log q_\theta(z|\eps)\right]\;.
\end{align*}
$\nabla_z\log q_\theta(z|\eps)$ can be evaluated by assumption.
\end{itemize}
\item
\uivi estimates the gradient of the \elbo by drawing $S$ samples from $q(\eps)$ and $q(u)$ (in practice, $S=1$):
\[
\nabla_\theta\calL(\theta) \approx \frac{1}{S}\sum_{s=1}^S\left(\nabla_z\log p(x,z)\big|_{z=h_\theta(u_s,\eps_s)}\nabla_\theta h_\theta(u_s;\eps_s) - \E_{q_\theta(\eps|z)}\left[\nabla_z\log q_\theta(z|\eps)\right]\big|_{z=h_\theta(u_s;\eps_s)}\nabla_\theta h_\theta(u_s;\eps_s)\right) \;.
\]
To estimate the inner expectation, samples are drawn from the reverse conditional $q_\theta(\eps|z)\propto q_\theta(z|\eps)q(\eps)$ using \mcmc. Exploiting the fact that $(z_s,\eps_s)$ comes from the joint $q_\theta(z,\eps)$, \uivi initializes the \mcmc at $\eps_s$ so no burn-in is required. A number of iterations are run to break the dependency between $\eps_s$ and the $\eps_s'$ that is used to estimate the inner expectation.

\end{itemize}

\subsection{Quality of approximation}

\todo: analyze the (best-case) approximation of \uivi. Questions:
\begin{enumerate}
\item
Approach? Probabilistic bound on KL as function of \elbo optimization iteration?
\item
How to deal with implicit mixing component? Do surrogate families simpler than neural networks help? What assumptions would be needed?
\item
Posterior contraction in terms of limiting data?
\end{enumerate}

\begin{itemize}

\item
Can we say something about \elbo maximizer $\hat\theta$, e.g.,
\begin{itemize}
\item
KL upper bound
\begin{align*}
\text{KL}(q_{\hat\theta}(z)\|p(z|x)) &= -\E_{q_{\hat\theta}(z)}\left[\log\frac{p(z|x)}{q_{\hat\theta}(z)}\right] \\
&= \E_{q_{\hat\theta}(z)}\left[\log\frac{q_{\hat\theta}(z)}{p(z|x)}\right] \\
&= \E_{q_{\hat\theta}(z)}\left[\log\frac{\E_{q(\eps)}\left[q_{\hat\theta}(z|\eps)\right]}{p(z|x)}\right]
\end{align*}
\item
\elbo lower bound
\begin{align*}
\calL(\hat\theta) &= \E_{q_{\hat\theta}(z)}\left[\log p(x,z) - \log q_{\hat\theta}(z)\right] \\
&= \E_{q_{\hat\theta}(z)}\left[\log p(x,z) - \log \E_{q(\eps)}\left[q_{\hat\theta}(z|\eps)\right]\right]
\end{align*}
\item
\href{https://stats.stackexchange.com/questions/308838/marginal-likelihood-derivation-for-normal-likelihood-and-prior}{Simple case}:

$X\sim N(Z,\sigma^2)$, prior $Z\sim N(\mu_0,\sigma^2_0)$, posterior $Z|X_{1:n}\sim N\left(\frac{\mu_0\sigma_0^{-2}+n\bar{X}\sigma^{-2}}{\sigma_0^{-2}+n\sigma^{-2}}, \sigma_1^2=\frac{1}{\sigma_0^{-2}+n\sigma^{-2}}\right)$.

Gaussian $q_\theta(z|\varepsilon)$:
\begin{align*}
\varepsilon &\sim N(0,1) \\
u &\sim N(0,1) \\
z &= h_\theta(u;\varepsilon) = \mu_\theta(\varepsilon) + \sigma_1u \\
\mu_\theta(\varepsilon) &= \theta + \varepsilon \\
z|\varepsilon &\sim N(\mu_\theta(\varepsilon), \sigma_1^2) = N(\theta + \varepsilon, \sigma_1^2) \\
z|\varepsilon,u &= \theta + \varepsilon + \sigma_1u \\
z &\sim N(\theta, \sigma_1^2+1) \\
z &\sim N\left(\E\left[\mu_\theta(\varepsilon)\right], \sigma_1^2+\mathrm{Var}\left(\mu_\theta(\varepsilon)\right)\right)
\end{align*}
This says that for this normal-normal model, the true posterior is not in our variational family, and no function $\mu_\theta(\varepsilon)$ is able to change that unless $\mu_\theta(\varepsilon)$ is constant. \todo: problem is that $\sigma$ in $h_\theta$ is misspecified. Learning both fixes issue?
\begin{align*}
z &= \mu_\theta(\varepsilon_1) + \sigma_\theta(\varepsilon_2)u \\
&\sim N\left(\E\left[\mu_\theta(\varepsilon_1)\right], \mathrm{Var}\left(\mu_\theta(\varepsilon_1)\right) + \mathrm{Var}\left(\sigma_\theta(\varepsilon_2)\right)\right)
\end{align*}
if learning independently.

\item
\href{https://www2.math.upenn.edu/~kazdan/508F10/convolution.pdf}{Convolution}: approximation identities

Kruijer (2020): convolution error

\item
Differential entropy not invariant under change of variables.

\end{itemize}
\end{itemize}

Approaches:
\begin{itemize}

\item
Question mainly boils down to how expressive is the implicit distributional family?

\item
KL between true posterior and variational distribution:
\begin{itemize}
\item
Analytic approach: normal-normal example below shows simple case where true posterior is in variational family and where it is not.
\item
More complicated attempt: come up with analytic $q_\theta(z)$ for more complex mixing (e.g., normalizing flow) but likely not generalizable as in general is intractable. Intention: for any well-behaved target and base, there exists a diffeomorphism that can turn the base into the target.
\item
\citet{Plummer:2021} provides probabilistic bounds on KL between true posterior and variational distribution given by a particular implicit model (non-linear latent variable model with a Gaussian process prior), and maybe posterior contraction to true density? Unclear how generalizable results are based on current understanding.
\end{itemize}

\item
Posterior contraction/measure of approximation of variational distribution and limiting posterior?

\item
Dimensionality? Is this just a problem of convergence/complexity?

\end{itemize}


\subsection{Gradient variance}

\todo are there scenarios where \uivi breaks down that other \vi methods may not?

\begin{itemize}

\item
Approximating posterior using Gaussian mixtures in high-dimensions.

\item
Reparameterization gradients and variances + other relevant references:
\begin{itemize}
\item
\href{https://arxiv.org/pdf/1610.02287.pdf}{The Generalized Reparameterization Gradient} (Ruiz 2016)
\item
\href{https://proceedings.neurips.cc/paper/2017/file/325995af77a0e8b06d1204a171010b3a-Paper.pdf}{Reducing Reparameterization Gradient Variance} (Miller 2017)
\item
\href{https://arxiv.org/pdf/1809.10330.pdf}{Variance reduction properties of the reparameterization trick} (Xu 2018): applies CLT over multiple samples so each partial derivative is approximately normal; Taylor expansions?
\item
\href{https://jmlr.org/papers/volume21/19-346/19-346.pdf}{Monte Carlo Gradient Estimation in Machine Learning} (Mohamed 2020)
\item
\href{http://www.svcl.ucsd.edu/courses/ece175/handouts/slides11.pdf}{EM Algorithm \& High Dimensional Data} (slides)
\item
BBVI, structured SIVI
\end{itemize}

\item
Rao-Blackwellization:
\begin{align*}
L(\bfu,\bfeps,\bfeps') &= \widehat{\nabla}_\theta\calL(\theta;\bfu,\bfeps,\bfeps') \\
L(\bfeps) &= \E_{\bfu,\bfeps'}[L(\bfu,\bfeps,\bfeps')] \\
\var(L(\bfu,\bfeps,\bfeps')) &= \var(L(\bfeps)) + \E[(L(\bfu,\bfeps,\bfeps')-L(\bfeps))^2] \\
& \geq \var(L(\bfeps))
\end{align*}
\begin{align*}
\var\left(L_\theta(\bfu,\bfeps,\bfeps_{1:m}')\right) &= \E_{\bfu,\bfeps,\bfeps_{1:m}}\left[\left(L_\theta(\bfu,\bfeps,\bfeps_{1:m}') - \E_{\bfu,\bfeps,\bfeps_{1:m}}\left[L_\theta(\bfu,\bfeps,\bfeps_{1:m}')\right]\right)^2\right] \\
&= \E_{\bfu,\bfeps,\bfeps_{1:m}}\left[\left(L_\theta(\bfu,\bfeps,\bfeps_{1:m}') - \E_{\bfu,\bfeps,\bfeps_{1:m}}\left[L_\theta(\bfu,\bfeps,\bfeps_{1:m}')\right]\right)^2\right] \\
&= \var\left(L_\theta(\bfeps_{1:m}')\right)
\end{align*}
\begin{align*}
\var\left(L_\theta(\bfeps_{1:m}')\right) &= \E_{\bfeps_{1:m}'}\left[\left(L_\theta(\bfeps_{1:m}')-\E_{\bfeps_{1:m}'}\left[L_\theta(\bfeps_{1:m}')\right]\right)^2\right] \\
&= \E_{\bfeps_{1:m}'}\left[\left(L_\theta(\bfeps_{1:m}')-\E_{\bfu,\bfeps,\bfeps_{1:m}'}\left[L_\theta(\bfu,\bfeps,\bfeps_{1:m}')\right]\right)^2\right] \\
&= \E_{\bfeps_{1:m}'}\left[\left(L_\theta(\bfeps_{1:m}')-L_\theta(\bfu,\bfeps,\bfeps_{1:m}')+L_\theta(\bfu,\bfeps,\bfeps_{1:m}')-\E_{\bfu,\bfeps,\bfeps_{1:m}'}\left[L_\theta(\bfu,\bfeps,\bfeps_{1:m}')\right]\right)^2\right] \\
&= \E_{\bfeps_{1:m}'}\left[\left(L_\theta(\bfeps_{1:m}')-L_\theta(\bfu,\bfeps,\bfeps_{1:m}')\right)^2\right] \\
&\quad - 2\E_{\bfeps_{1:m}'}\left[\left(L_\theta(\bfeps_{1:m}')-L_\theta(\bfu,\bfeps,\bfeps_{1:m}')\right)\left(L_\theta(\bfu,\bfeps,\bfeps_{1:m}')-\E_{\bfu,\bfeps,\bfeps_{1:m}'}\left[L_\theta(\bfu,\bfeps,\bfeps_{1:m}')\right]\right)\right] \\
&\quad + \var\left(L_\theta(\bfu,\bfeps,\bfeps_{1:m}')\right)
\end{align*}
\todo Can we show lower bound depends on dimension $d$ of $\bfz$ somehow assuming that $q_\theta(\bfz)=p(\bfz|\bfx)$ (or very close, e.g., in convolution family), meaning that in high dimensions, variance is still high even at the optimum? Take $\bfeps$ to be of dimension $d$ and take mapping function as inverse CDF (this is just the posterior itself? i.e., $\var_{q_\theta}(\bfz)=\var_{p_{\bfz|\bfx}}(\bfz)$ with $\sigma=0$). Assume that $q_\theta(\bfz)\approx p(\bfz|\bfx)$ so that variances are approximately equal?

\item
\[
\widehat{\nabla}_\theta\calL(\theta) = \frac{1}{n}\sum_{i=1}^n \left(\nabla_\bfz\log p(\bfx,\bfz)\big|_{\bfz=h_\theta(\bfu_i;\bfeps_i)} - \frac{1}{m}\sum_{j=1}^m \nabla_\bfz\log q_\theta(\bfz|\bfeps_j')\big|_{\bfz=h_\theta(\bfu_i;\bfeps_i)} \right)\nabla_\theta h_\theta(\bfu_i;\bfeps_i)
\]
\todo Under above assumptions?
\begin{align*}
L(\bfeps) &= \E_\bfu\left[\nabla_\bfz\log p(\bfx,h_\theta(\bfu;\bfeps))\nabla_\theta h_\theta(\bfu;\bfeps)\right] - \E_{\bfu,\bfeps'}\left[\nabla_\bfz\log q_\theta(h_\theta(\bfu;\bfeps)|\bfeps') \nabla_\theta h_\theta(\bfu;\bfeps)\right] \\
&= \E_\bfu\left[\nabla_\bfz\log p(\bfx,h_\theta(\bfu;\bfeps))\nabla_\theta h_\theta(\bfu;\bfeps)\right] - \E_\bfu\left[\nabla_\bfz\E_{\bfeps'}\left[\log q_\theta(h_\theta(\bfu;\bfeps)|\bfeps')\right] \nabla_\theta h_\theta(\bfu;\bfeps)\right] \\
&\geq \E_\bfu\left[\nabla_\bfz\log p(\bfx,h_\theta(\bfu;\bfeps))\nabla_\theta h_\theta(\bfu;\bfeps)\right] - \E_\bfu\left[\nabla_\bfz\log q_\theta(h_\theta(\bfu;\bfeps)) \nabla_\theta h_\theta(\bfu;\bfeps)\right] \\
&= 0 \\
\var(L(\bfeps)) &= \var\left(\E_\bfu\left[\nabla_\bfz\log p(\bfx,h_\theta(\bfu;\bfeps))\nabla_\theta h_\theta(\bfu;\bfeps)\right]\right) + \var\left(\E_\bfu\left[\nabla_\bfz\E_{\bfeps'}\left[\log q_\theta(h_\theta(\bfu;\bfeps)|\bfeps')\right] \nabla_\theta h_\theta(\bfu;\bfeps)\right]\right) \\
&\quad - \cov\left(\E_\bfu\left[\nabla_\bfz\log p(\bfx,h_\theta(\bfu;\bfeps))\nabla_\theta h_\theta(\bfu;\bfeps)\right]\left(\E_\bfu\left[\nabla_\bfz\E_{\bfeps'}\left[\log q_\theta(h_\theta(\bfu;\bfeps)|\bfeps')\right]\nabla_\theta h_\theta(\bfu;\bfeps)\right]\right)^\T\right)
\end{align*}
\todo inequality of $L(\bfeps)$ not useful?
\begin{align*}
L(\bfeps) &= \E_\bfu\left[\nabla_\bfz\log p(\bfx,h_\theta(\bfu;\bfeps))\nabla_\theta h_\theta(\bfu;\bfeps)\right] - \E_\bfu\left[\nabla_\bfz\E_{\bfeps'}\left[\log q_\theta(h_\theta(\bfu;\bfeps)|\bfeps')\right] \nabla_\theta h_\theta(\bfu;\bfeps)\right] \\
&= \E_\bfu\left[\nabla_\bfz\log p(\bfx,h_\theta(\bfu;\bfeps))\nabla_\theta h_\theta(\bfu;\bfeps)\right] - \E_\bfu\left[\nabla_\bfz\log q_\theta(h_\theta(\bfu;\bfeps)) \nabla_\theta h_\theta(\bfu;\bfeps)\right] \\
&= \E_\bfu\left[\nabla_\bfz\log p(h_\theta(\bfu;\bfeps)|\bfx)\nabla_\theta h_\theta(\bfu;\bfeps)\right] - \E_\bfu\left[\nabla_\bfz\log q_\theta(h_\theta(\bfu;\bfeps)) \nabla_\theta h_\theta(\bfu;\bfeps)\right]
\end{align*}
If have $\theta$ such that $q_\theta(\bfz)=p(\bfz|\bfx)$,
\begin{align*}
L(\bfeps_{1:m}') &= \E_{\bfu,\bfeps}\left[\nabla_\bfz\log p(\bfx,h_\theta(\bfu;\bfeps))\nabla_\theta h_\theta(\bfu;\bfeps)\right] - \frac{1}{m}\sum_{j=1}^m\E_{\bfu,\bfeps}\left[\nabla_\bfz\log q_\theta(h_\theta(\bfu;\bfeps)|\bfeps_j') \nabla_\theta h_\theta(\bfu;\bfeps)\right] \\
&= \E_{\bfu,\bfeps}\left[\nabla_\bfz\log p(\bfx,h_\theta(\bfu;\bfeps))\nabla_\theta h_\theta(\bfu;\bfeps)\right] - \frac{1}{m}\sum_{j=1}^m\E_{\bfu,\bfeps}\left[\nabla_\theta\log q_\theta(h_\theta(\bfu;\bfeps)|\bfeps_j')\right] \\
&= \E_{\bfu,\bfeps}\left[\nabla_\bfz\log p(\bfx,h_\theta(\bfu;\bfeps))\nabla_\theta h_\theta(\bfu;\bfeps)\right] - \frac{1}{m}\sum_{j=1}^m\E_{\bfu,\bfeps}\left[\nabla_\theta\log q_\theta(\bfz|\bfeps_j')\big|_{\bfz=h_\theta(\bfu;\bfeps)}\right]
\end{align*}
Given $\theta$ and $\bfeps_j'$, second term is expected score but unless $q_\theta(\bfz|\bfeps')=p(\bfz|\bfx)$ ($p$ must be Gaussian), expectation is non-zero?
\begin{align*}
\var(L(\bfeps_{1:m}')_i) &= \frac{1}{m}\var\left(\E_{\bfu,\bfeps}\left[\nabla_{\theta_i}\log q_\theta(\bfz|\bfeps')\big|_{\bfz=h_\theta(\bfu;\bfeps)}\right]\right) \\
&= \frac{1}{m}\E_{\bfeps'}\left[\E_{\bfu,\bfeps}\left[\nabla_{\theta_i}\log q_\theta(\bfz|\bfeps')\big|_{\bfz=h_\theta(\bfu;\bfeps)}\right]^2\right] \\
&= \frac{1}{m}\E_{\bfeps'}\left[\E_{\bfu,\bfeps}\left[\left(\Sigma_\theta(\bfeps')^{-1}(h_\theta(\bfu;\bfeps)-\mu_\theta(\bfeps'))\right)^\T\nabla_{\theta_i} h_\theta(\bfu;\bfeps)\right]^2\right] \\
&= \frac{1}{m}\E_{\bfeps'}\left[\E_{\bfu,\bfeps}\left[\left(\Sigma_\theta(\bfeps')^{-1}(\left(\mu_\theta(\bfeps)+\Sigma_\theta(\bfeps)\bfu\right)-\mu_\theta(\bfeps'))\right)^\T\nabla_{\theta_i} h_\theta(\bfu;\bfeps)\right]^2\right] \\
&= \frac{1}{m}\E_{\bfeps'}\left[\E_{\bfu,\bfeps}\left[\left(\Sigma_\theta(\bfeps')^{-1}(\mu_\theta(\bfeps)-\mu_\theta(\bfeps'))+\Sigma_\theta(\bfeps')^{-1}\Sigma_\theta(\bfeps)\bfu\right)^\T\nabla_{\theta_i} h_\theta(\bfu;\bfeps)\right]^2\right]
\end{align*}
If $\Sigma$ independent of $\bfeps$, then
\begin{align*}
\var(L(\bfeps_{1:m}')_i) &= \frac{1}{m}\E_{\bfeps'}\left[\E_{\bfu,\bfeps}\left[\left(\Sigma^{-1}(\mu_\theta(\bfeps)-\mu_\theta(\bfeps'))+\bfu\right)^\T\nabla_{\theta_i} h_\theta(\bfu;\bfeps)\right]^2\right]
\end{align*}
Can simplify further if $\Sigma$ diagonal but is there a point?

Notes:
\begin{enumerate}
\item
$\bfeps'$ sampled from $q(\bfeps|\bfz)$ via MCMC, but variance w.r.t. $q(\bfeps)$ considered here?
\item
Dimension of $\bfz$ shows up as score
\item
$\E_{\bfu,\bfeps,\bfeps'}\left[\nabla_{\theta_i}\log q_\theta(\bfz|\bfeps')\big|_{\bfz=h_\theta(\bfu;\bfeps)}\right]=0$
\item
Variance is variance of expected score of potentially high-dimensional distribution where parameters are changing according to $\bfeps$ but map to parameters of distribution is fixed due to fixed $\theta$
\end{enumerate}

\item
\todo: start with finite mixture, i.e.,
\begin{align*}
q_\theta(\bfz) &= \sum_{i=1}^Kq_\theta(\bfz|\bfeps_k)q(\bfeps_k) \\
\bfz &= \mu(\bfeps_k) + \sigma(\bfeps_k)\bfu \\
\bfu &\sim \calN(0,1) \\
\bfeps &\sim q(\bfeps)
\end{align*}
Show variance depends on $K$, and somehow $K$ is known to be exponential in $d$?

\iffalse
\item
If mixing distribution is discrete, we may have label-switching problems, particularly with \mcmc? \citep{Chung:2004}

Example: $q(\bfeps)=\mathrm{Bernoulli}(0.5)$. Then in normal reparameterization, we end up with parameters $\{\mu_1,\Sigma_1\}$, $\{\mu_2,\Sigma_2\}$. Can we estimate these following \elbo gradient or do we get degeneracy somehow?
\fi

\end{itemize}


\subsection{Scratch notes}

$\hat{\theta}=\frac{\mu_0\sigma_0^{-2}+n\bar{X}\sigma^{-2}}{\sigma_0^{-2}+n\sigma^{-2}}$:
\begin{align*}
\mathrm{KL}(q_\theta(z)\|p(z|x)) &= - \int q_\theta(z)\log\frac{p(z|x)}{q_\theta(z)}dz \\
&= - \int q_\theta(z)\log p(z|x) + \int q_\theta(z)\log q_\theta(z)dz 
\end{align*}
\begin{align*}
u &\sim N(0,1) \\
z &= h_\theta(u;\varepsilon) = \mu_\theta(\varepsilon) + \sigma_1u \\
\mu_\theta(\varepsilon) &= \theta + \varepsilon \\
u &= h_\theta^{-1}(z;\varepsilon) = \sigma_1^{-1}(z-\mu_\theta(\varepsilon)) \\
\nabla_z h_\theta^{-1}(z;\varepsilon) &= \sigma_1^{-1} \\
q_\theta(z|\varepsilon) &= q_u(h_\theta^{-1}(z;\varepsilon))\sigma_1^{-1} \\
q_\theta(z) &= \int q_\theta(z|\varepsilon)q(\varepsilon)d\varepsilon \\
&= \int \sigma_1^{-1}q_u\left(\sigma_1^{-1}\left(z-\mu_\theta(\varepsilon)\right)\right)q(\varepsilon)d\varepsilon \\
&= \int \sigma_1^{-1}q_u\left(\sigma_1^{-1}\left(z-\theta-\varepsilon\right)\right)q(\varepsilon)d\varepsilon \\
&= \int \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{1}{2}\left(\sigma_1^{-2}(z-\theta-\varepsilon)^2\right)\right)q(\varepsilon)d\varepsilon \\
&= \int \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{1}{2\sigma_1^2}\left((z-\theta)^2-2(z-\theta)\varepsilon+\varepsilon^2\right)\right)q(\varepsilon)d\varepsilon \\
&= \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{1}{2\sigma_1^2}(z-\theta)^2\right) \int \exp\left(-\frac{1}{2\sigma_1^2}\left(-2(z-\theta)\varepsilon+\varepsilon^2\right)\right)q(\varepsilon)d\varepsilon \\
&= \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{1}{2\sigma_1^2}(z-\theta)^2\right) \int \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2\sigma_1^2}\left(-2(z-\theta)\varepsilon+\varepsilon^2\right)-\frac{1}{2}\varepsilon^2\right)d\varepsilon
\end{align*}
Posterior exact when ?

\begin{itemize}

\item
If $h_\theta$ monotonic, invertible:
\begin{align*}
z &= h_\theta(u;\bfeps) \\
q_\theta(z|\bfeps) &= q_u\left(h_\theta^{-1}(z;\bfeps)\right)\left|\nabla_zh_\theta^{-1}(z;\bfeps)\right| \\
q_\theta(z) &= \int q_u\left(h_\theta^{-1}(z;\bfeps)\right)\left|\nabla_zh_\theta^{-1}(z;\bfeps)\right| q(\bfeps)d\bfeps
\end{align*}
\todo: normalizing flow literature? Restrict $h_\theta$ to be independent of $\bfeps$ (e.g., linear flows)?

\end{itemize}


\newpage


\section{Semi-implicit variational inference}

Based on \citet{Yin:2018}.
\\

\sivi is addresses the issues of classical VI attributed to the requirement of a conditionally conjugate variational family by relaxing this requirement to allow for implicit distributional families from which samples can be drawn. This implicit family consists of hierarchical distributions with a mixing parameter. While the distribution conditioned on the mixing parameter is required to be analytical and reparameterizable, the mixing distribution can be arbitrarily complex. The use of such a variational family also addresses the problems of conventional mean-field families as dependencies between the latent variables can be introduced through the mixing distribution.
\\

The objective in \sivi is a surrogate \elbo that is only exact asymptotically and otherwise a lower bound of the \elbo~\citep{Molchanov:2019}. Like in black box VI, the gradients are rewritten as expectations and estimated via Monte Carlo samples.
\\

\citet{Molchanov:2019} extends \sivi to doubly \sivi for variational inference and variational learning in which both the variational posterior and the prior are semi-implicit distributions. They also show that the \sivi objective is a lower bound of the \elbo.
\\

\citet{Molchanova:2019} and \citet{Moens:2021} comment that \sivi and \uivi struggle in high-dimensional regimes. \mcmc methods also have high variance \citep{Moens:2021}.
\\

\citet{Moens:2021} introduce compositional implicit variational inference (CI-VI), which rewrites the \sivi \elbo as a compositional nested form $\E_\nu\left[f_\nu\left(\E_\omega\left[g_\omega(\theta)\right]\right)\right]$. The gradient involves estimating the nested expectations, for which a simple Monte-Carlo estimator would be biased. CI-VI uses an extrapolation-smoothing scheme for which the bias converges to zero with iterations. In practice, the gradient involves matrix-vector products that are expensive but can be approximated via sketching techniques. Under certain assumptions, convergence of the CI-VI algorithm is proved in terms of the number of oracle calls needed to convergence (\todo).


\section{Hierarchical variational inference}

Based on \citet{Ranganath:2016}.
\\

Predating \sivi and \uivi, \hvm first(?) addressed the restricted variational family issue of classical VI by using a hierarchical variational distribution which is enabled by \bbvi. \hvm considers a mean-field variational likelihood and a variational prior that is differentiable (e.g., a mixture or a normalizing flow). \hvm also optimizes a lower bound of the \elbo that is constructed using a recursive variational distribution that approximates the variational prior.


\newpage


\section{Theoretical guarantees for implicit VI}

Based on \citet{Plummer:2021}.
\\

\todo: Considers non-linear latent variable model (NL-LVM)
\begin{align*}
z &= \mu(\varepsilon) + u \\
u &\sim N(0,\sigma^2) \\
\varepsilon &\sim U(0,1) \\
\mu &\sim \Pi_\mu \\
\sigma &\sim \Pi_\sigma
\end{align*}
where $\Pi_\mu$ and $\Pi_\sigma$ are priors. Can write as
\begin{align*}
z &= \mu(\varepsilon) + \sigma u \\
u &\sim N(0,1)
\end{align*}
This leads to density
\begin{align*}
f_{\mu,\sigma}(z) = f(z;\mu,\sigma) &= \int_0^1\phi_\sigma(y-\mu(\bfeps))d\bfeps \\
&= \int \phi_\sigma(y-t)d\nu_\mu(t)
\end{align*}
where $\phi_\sigma$ is the density of a N$(0,\sigma^2\bfI_d)$ distribution, and $\nu_\mu=\lambda\circ\mu^{-1}$ the image measure where $\lambda$ is the Lebesgue measure and $\mu:[0,1]\rightarrow\bbR$. The second form is a convolution with a Gaussian kernel and suggests that $f_{\mu,\sigma}$ is flexible depending on the choice of $\mu$. Under certain assumptions on $f_0$, it is known that $\phi_\sigma*f_0$ can approximate $f_0$ arbitrarily close as bandwidth $\sigma\rightarrow 0$. This should hold for \uivi under particular choices of the reparameterization and mixing distributions.
\\

A Gaussian process latent variable model puts a GP prior for the transfer function $\mu$. (Theorem~3.1) If $\Pi_\mu$ has full sup-norm support on $C[0,1]$ and $\Pi_\sigma$ has full support on $[0,\infty)$, then the $L_1$ support of the induced prior $\Pi=(\Pi_\mu\otimes\Pi_\sigma)\circ f_{\mu,\sigma}^{-1}$ contains all densities which have a first finite moment and are non-zero almost everywhere on their support.
\\

\todo: posterior contraction says expected divergence of posterior density and true density goes to 0 given observations of the response $z$. The response in our case is the latent variable. Can this work with our observations $x$? This likely does not apply as the true posterior is the one that changes and the variational distribution is only approximating the true posterior. If the true posterior is always in the family and we can approximate it exactly then posterior contraction follows standard Bayesian results.
\\

Introduces Gaussian process implicit \vi (GP-IVI), which uses a finite mixture of uniform mixing distributions. \todo: transfer function not necessarily GP? Has probabilistic bound on error of best approximation to posterior and an $\alpha$-variational Bayes risk bound.
\\

For simple normal-normal model, KL divergence for true normal model and true posterior converges weakly to a $\chi_1^2$ and not to 0.


\newpage


\section{Other references}

VI review:
\begin{itemize}
\item
\href{https://s3.amazonaws.com/ieeecs.cdn.csdl.content/trans/tp/2019/08/08588399.pdf?AWSAccessKeyId=ASIA2Z6GPE73ITPNF4HV&Expires=1646432232&Signature=%2BwbeAhSyM%2FMqzaAhLaunhuq30BI%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEP7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIGoE%2FVHY77X2Kpq0J47Ic29AHEKJ%2F%2FS0foSv98aaP%2F%2BxAiAYu9FImOV6wjyu7v9HkfGlfyytEvSd2wujoh7wIivaTyqdAghmEAAaDDc0MjkwODcwMDY2MiIMYEqAQr9DL2OWCGlbKvoB58nRjaBdUqwysmhAcnlM2JP60usEfHlgP0Bn2xGYd1kZJPiEKzmA%2F76Bo7YRZ2z%2FpCEaHPV7aFI6ikqSiJToz5QlGqPUQp5GF6qR5TSKEJ5oMTODfVyUt5xdcYOww012wn%2FyqAE3T9hRTF%2FyxhZeY6Q1xrj3Bzu8p0jP7NO30Xoje2Oct8JA6sOwhgFEL5rjtIpPoiiAywnSeWz8Ia8ra4tpCOPbG8T3SLq%2BFE4SS3BrviPuGWH0b0vr2EzBSZEoWFySA87d27hh99%2B%2BsLKLvcu4u3VGRUWb%2FpwIYAT88MBxL1RhCazyRj4eByB%2FsbVAhmGQ0bYAYYZT1TDqhYqRBjqbAcZHTiSLr48H4Ha2O6bVtVfH9%2B8e%2FmWAg2joY8DUAAvhdiGeqGLdPfgpNE0%2BpXfJI%2BKR8LeVrJvtfpYI3LV2RQn%2B3y2cnzIrosiNxE68tL4U%2BsW4uor4O6eZ8qvF%2Bvr7ncAy4rulMRlaqpgY%2FehIGk5UxsUPTr4zC%2BYNJXlKu28DlNal14YG5ugTN3VC5sgxL%2B%2Bq%2BEp8ibbwRzSf}{Advances in Variational Inference} (2019)
\item
\href{https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773}{Variational Inference: A Review for Statisticians} (2017)
\item
\href{https://arxiv.org/pdf/1401.0118.pdf}{Black Box Variational Inference} (2013): dominated convergence theorem used to push gradient into expectation
\end{itemize}
Possibly related VI approaches/of interest
\begin{itemize}
\item
\href{https://proceedings.mlr.press/v80/yin18b/yin18b.pdf}{Semi-Implicit Variational Inference} (2018)

\href{https://proceedings.mlr.press/v89/molchanov19a/molchanov19a.pdf}{Doubly Semi-Implicit Variational Inference} (2019)

\href{https://openreview.net/pdf?id=HkxStk34Kr}{Structured Semi-Implicit Variational Inference} (2019): mentions that previous methods scale exponentially with dimension of the latent variables. Imposes that the high-dimensional semi-implicit distribution factorizes into a product of low-dimensional conditional semi-implicit distributions and shows that the resulting entropy bound is tighter than that of \sivi's and consequently a tighter \elbo objective.

\href{https://arxiv.org/pdf/2101.06070.pdf}{Efficient Semi-Implicit Variational Inference
} (2021)
\item
\href{https://arxiv.org/pdf/1702.08235.pdf}{Variational Inference using Implicit Distributions} (2017): implicit with density ratio estimation?
\item
\href{https://proceedings.neurips.cc/paper/2019/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf}{Importance Weighted Hierarchical
Variational Inference} (2019)
\item
\href{https://jmlr.org/papers/volume22/19-1028/19-1028.pdf}{Normalizing Flows for Probabilistic Modeling and Inference} (2021)

\href{https://arxiv.org/pdf/2002.09547.pdf}{Stochastic Normalizing Flows} (2020)
\item
Implicit VI:

\href{https://bayesgroup.github.io/bmml_sem/2018/Molchanov_Implicit%20Models_2018_2.pdf}{Variational Inference with
Implicit Models} (2018; slides)

\href{https://arxiv.org/pdf/2010.12995.pdf}{Implicit Variational Inference: the Parameter and the Predictor Space} (2020): optimizing over predictor space rather than parameter space?
\end{itemize}
Theory/analysis
\begin{itemize}
\item
\href{https://proceedings.mlr.press/v130/plummer21a/plummer21a.pdf}{Statistical Guarantees for Transformation Based Models with Applications to Implicit Variational Inference} (2021)

\href{https://oaktrust.library.tamu.edu/bitstream/handle/1969.1/195122/PLUMMER-DISSERTATION-2021.pdf?sequence=1&isAllowed=y}{Statistical and Computational Properties of Variational Inference} (2021; thesis)
\item
\href{https://knowledge.uchicago.edu/record/2221?ln=en}{Theoretical Guarantees of Variational Inference and Its Applications} (2020; thesis)

\href{https://arxiv.org/pdf/1710.03266.pdf}{$\alpha$-Variational Inference with Statistical Guarantees} (2018): a particular variational family with theoretical guarantees
\item
\href{https://tel.archives-ouvertes.fr/tel-02893465/document}{Contributions to the theoretical study of variational inference and robustness} (2020; thesis)
\item
\href{https://par.nsf.gov/servlets/purl/10180931}{On Statistical Optimality of Variational Bayes} (2018): general guarantees for variational estimates as approximations for true data-generating parameter for MF-VI using variational risk bounds?

\href{https://math.unm.edu/~skripka/workshop_high_dim/Anirban_Talks.pdf}{Statistical guarantees for variational Bayes} (2021; slides)
\item
\href{https://arxiv.org/pdf/2010.09540.pdf}{Statistical Guarantees and Algorithmic Convergence Issues of Variational Boosting} (2020)
\item
\href{https://proceedings.neurips.cc/paper/2020/file/7cac11e2f46ed46c339ec3d569853759-Paper.pdf}{Robust, Accurate Stochastic Optimization for Variational Inference} (2020) -- iterates as \mcmc?
\item
\href{https://arxiv.org/pdf/1908.04847.pdf}{Convergence Rates of Variational Inference in Sparse Deep Learning} (2019)

\href{https://arxiv.org/pdf/1902.05068.pdf}{On the Convergence of Extended Variational Inference for Non-Gaussian Statistical Models} (2020)

\end{itemize}


\newpage

\bibliographystyle{plainnat}
\bibliography{../report/qp}

\end{document}