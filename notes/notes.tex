\documentclass[10pt]{article}
\input{../report/header}
\input{../report/defs}


\begin{document}

\tableofcontents

\newpage

\section{Unbiased Implicit Variational Inference}

Based on \citet{Titsias:2019}.

\begin{itemize}

\item
Authors introduce unbiased implicit variational inference (UIVI) that defines a flexible variational family. Like semi-implicit variational inference (SIVI), UIVI uses an implicit variational distribution $q_\theta(z)=\int q_\theta(z|\varepsilon)q(\varepsilon)d\varepsilon$ where $q_\theta(z|\varepsilon)$ is a reparameterizable distribution whose parameters can be outputs of some neural network $g$, i.e., $q_\theta(z|\varepsilon)=h(u;g(\varepsilon;\theta))$ with $u\sim q(u)$. Under two assumptions on the conditional $q_\theta(z|\varepsilon)$, the ELBO can be approximated via Monte Carlo sampling. In particular, the entropy component of the ELBO can be rewritten as an expectation w.r.t. the reverse conditional $q_\theta(\varepsilon|z)$. Efficient approximation of this expectation w.r.t. the reverse conditional is done by reusing samples from approximating the main expectation to initialize a MCMC sampler.

\item
Questions: \todo
\begin{enumerate}
\item
Can the gradient be pushed into the expectation? (Section~2.2)
\end{enumerate}

\item
In \sivi, the variational distribution $q_\theta(z)$ is defined as
\[
q_\theta(z) = \int q_\theta(z|\eps)q(\eps)d\eps
\]
where $\eps\sim q(\eps)$.

\item
\uivi:
\begin{itemize}
\item
Like \sivi, \uivi uses an implicit variational distribution $q_\theta(z)$ whose density cannot be evaluated but from which samples can be drawn. Unlike \sivi, \uivi directly maximizes the \elbo rather than a lower bound.
\item
The dependence of $q_\theta(z|\eps)$ on $\eps$ can be arbitrarily complex. \citet{Titsias:2019} take the parameters of a reparameterizable distribution (Assumption~1) as the output of a neural network with parameters $\theta$ that takes $\eps$ as input, i.e.,
\[
z = h(u; g_\theta(\eps)) = h_\theta(u;\eps)
\]
where $u\sim q(u)$ and $g_\theta$ is some neural network. It is also assumed that $\nabla_z\log q_\theta(z|\eps)$ can be evaluated (Assumption~2).
\item
The gradient of the \elbo is given by
\begin{align*}
\nabla_\theta\calL(\theta) &= \nabla_\theta\E_{q_\theta(z)}\left[\log p(x,z) - \log q_\theta(z)\right] \\
&= \nabla_\theta\int\left(\log p(x,z) - \log q_\theta(z)\right)q_\theta(z)dz \\
&= \int\nabla_\theta\left(\left(\log p(x,z) - \log q_\theta(z)\right)q_\theta(z)\right)dz \\
&= \int\nabla_\theta\left(\left(\log p(x,z) - \log q_\theta(z)\right)\int q_\theta(z|\eps)q(\eps)d\eps\right)dz \\
&= \int\int\nabla_\theta\left(\left(\log p(x,z) - \log q_\theta(z)\right)\big|_{z=h_\theta(u;\eps)}\right)q(u)q(\eps)d\eps du \\
&= \E_{q(\eps)q(u)}\left[\nabla_z\log p(x,z)\big|_{z=h_\theta(u;\eps)}\nabla_\theta h_\theta(u;\eps)\right] - \E_{q(\eps)q(u)}\left[\nabla_z\log q_\theta(z)\big|_{z=h_\theta(u;\eps)}\nabla_\theta h_\theta(u;\eps)\right] \;.
\end{align*}
(\todo: where is $\E_{q_\theta(z)}\left[\nabla_\theta\log q_\theta(z)\right]=0$ applied?) (Gradient can be pushed into expectation using DCT.) As $\nabla_z\log q_\theta(z)$ cannot be evaluated, this gradient is rewritten as an expectation using the log-deritative identity: $\nabla_x\log f(x) = \frac{1}{f(x)}\nabla_x f(x)$:
\begin{align*}
\nabla_z\log q_\theta(z) &= \frac{1}{q_\theta(z)}\nabla_zq_\theta(z) \\
&= \frac{1}{q_\theta(z)}\nabla_z\int q_\theta(z|\eps)q(\eps)d\eps \\
&= \frac{1}{q_\theta(z)}\int \nabla_z q_\theta(z|\eps)q(\eps)d\eps \\
&= \frac{1}{q_\theta(z)}\int q_\theta(z|\eps)q(\eps)\nabla_z\log q_\theta(z|\eps)d\eps \\
&= \int q_\theta(\eps|z)\nabla_z\log q_\theta(z|\eps)d\eps \\
&= \E_{q_\theta(\eps|z)}\left[\nabla_z\log q_\theta(z|\eps)\right]\;.
\end{align*}
$\nabla_z\log q_\theta(z|\eps)$ can be evaluated by assumption.
\end{itemize}
\item
\uivi estimates the gradient of the \elbo by drawing $S$ samples from $q(\eps)$ and $q(u)$ (in practice, $S=1$):
\[
\nabla_\theta\calL(\theta) \approx \frac{1}{S}\sum_{s=1}^S\left(\nabla_z\log p(x,z)\big|_{z=h_\theta(u_s,\eps_s)}\nabla_\theta h_\theta(u_s;\eps_s) - \E_{q_\theta(\eps|z)}\left[\nabla_z\log q_\theta(z|\eps)\right]\big|_{z=h_\theta(u_s;\eps_s)}\nabla_\theta h_\theta(u_s;\eps_s)\right) \;.
\]
To estimate the inner expectation, samples are drawn from the reverse conditional $q_\theta(\eps|z)\propto q_\theta(z|\eps)q(\eps)$ using \mcmc. Exploiting the fact that $(z_s,\eps_s)$ comes from the joint $q_\theta(z,\eps)$, \uivi initializes the \mcmc at $\eps_s$ so no burn-in is required. A number of iterations are run to break the dependency between $\eps_s$ and the $\eps_s'$ that is used to estimate the inner expectation.

\end{itemize}

\subsection{Analysis}

\todo: analyze the (best-case) approximation of \uivi. Questions:
\begin{enumerate}
\item
Approach? Probabilistic bound on KL as function of \elbo optimization iteration?
\item
How to deal with implicit mixing component? Do surrogate families simpler than neural networks help? What assumptions would be needed?
\item
Posterior contraction in terms of limiting data?
\end{enumerate}

\begin{itemize}

\item
Can we say something about \elbo maximizer $\hat\theta$, e.g.,
\begin{itemize}
\item
KL upper bound
\begin{align*}
\text{KL}(q_{\hat\theta}(z)\|p(z|x)) &= -\E_{q_{\hat\theta}(z)}\left[\log\frac{p(z|x)}{q_{\hat\theta}(z)}\right] \\
&= \E_{q_{\hat\theta}(z)}\left[\log\frac{q_{\hat\theta}(z)}{p(z|x)}\right] \\
&= \E_{q_{\hat\theta}(z)}\left[\log\frac{\E_{q(\eps)}\left[q_{\hat\theta}(z|\eps)\right]}{p(z|x)}\right]
\end{align*}
\item
\elbo lower bound
\begin{align*}
\calL(\hat\theta) &= \E_{q_{\hat\theta}(z)}\left[\log p(x,z) - \log q_{\hat\theta}(z)\right] \\
&= \E_{q_{\hat\theta}(z)}\left[\log p(x,z) - \log \E_{q(\eps)}\left[q_{\hat\theta}(z|\eps)\right]\right]
\end{align*}
\item
\href{https://stats.stackexchange.com/questions/308838/marginal-likelihood-derivation-for-normal-likelihood-and-prior}{Simple case}:

$X\sim N(Z,\sigma^2)$, prior $Z\sim N(\mu_0,\sigma^2_0)$, posterior $Z|X_{1:n}\sim N\left(\frac{\mu_0\sigma_0^{-2}+n\bar{X}\sigma^{-2}}{\sigma_0^{-2}+n\sigma^{-2}}, \sigma_1^2=\frac{1}{\sigma_0^{-2}+n\sigma^{-2}}\right)$.

Gaussian $q_\theta(z|\varepsilon)$:
\begin{align*}
\varepsilon &\sim N(0,1) \\
u &\sim N(0,1) \\
z &= h_\theta(u;\varepsilon) = \mu_\theta(\varepsilon) + \sigma_1u \\
\mu_\theta(\varepsilon) &= \theta + \varepsilon \\
z|\varepsilon &\sim N(\mu_\theta(\varepsilon), \sigma_1^2) = N(\theta + \varepsilon, \sigma_1^2) \\
z|\varepsilon,u &= \theta + \varepsilon + \sigma_1u \\
z &\sim N(\theta, \sigma_1^2+1) \\
z &\sim N\left(\E\left[\mu_\theta(\varepsilon)\right], \sigma_1^2+\mathrm{Var}\left(\mu_\theta(\varepsilon)\right)\right)
\end{align*}
This says that for this normal-normal model, the true posterior is not in our variational family, and no function $\mu_\theta(\varepsilon)$ is able to change that unless $\mu_\theta(\varepsilon)$ is constant. \todo: problem is that $\sigma$ in $h_\theta$ is misspecified. Learning both fixes issue?
\begin{align*}
z &= \mu_\theta(\varepsilon_1) + \sigma_\theta(\varepsilon_2)u \\
&\sim N\left(\E\left[\mu_\theta(\varepsilon_1)\right], \mathrm{Var}\left(\mu_\theta(\varepsilon_1)\right) + \mathrm{Var}\left(\sigma_\theta(\varepsilon_2)\right)\right)
\end{align*}
if learning independently. \todo Do Gaussian process/convolution ideas \citep{Plummer:2021} apply when reparameterized distribution is additive?

\item
Differential entropy not invariant under change of variables.

\end{itemize}

\subsubsection{Scratch notes}

$\hat{\theta}=\frac{\mu_0\sigma_0^{-2}+n\bar{X}\sigma^{-2}}{\sigma_0^{-2}+n\sigma^{-2}}$:
\begin{align*}
\mathrm{KL}(q_\theta(z)\|p(z|x)) &= - \int q_\theta(z)\log\frac{p(z|x)}{q_\theta(z)}dz \\
&= - \int q_\theta(z)\log p(z|x) + \int q_\theta(z)\log q_\theta(z)dz 
\end{align*}
\begin{align*}
u &\sim N(0,1) \\
z &= h_\theta(u;\varepsilon) = \mu_\theta(\varepsilon) + \sigma_1u \\
\mu_\theta(\varepsilon) &= \theta + \varepsilon \\
u &= h_\theta^{-1}(z;\varepsilon) = \sigma_1^{-1}(z-\mu_\theta(\varepsilon)) \\
\nabla_z h_\theta^{-1}(z;\varepsilon) &= \sigma_1^{-1} \\
q_\theta(z|\varepsilon) &= q_u(h_\theta^{-1}(z;\varepsilon))\sigma_1^{-1} \\
q_\theta(z) &= \int q_\theta(z|\varepsilon)q(\varepsilon)d\varepsilon \\
&= \int \sigma_1^{-1}q_u\left(\sigma_1^{-1}\left(z-\mu_\theta(\varepsilon)\right)\right)q(\varepsilon)d\varepsilon \\
&= \int \sigma_1^{-1}q_u\left(\sigma_1^{-1}\left(z-\theta-\varepsilon\right)\right)q(\varepsilon)d\varepsilon \\
&= \int \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{1}{2}\left(\sigma_1^{-2}(z-\theta-\varepsilon)^2\right)\right)q(\varepsilon)d\varepsilon \\
&= \int \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{1}{2\sigma_1^2}\left((z-\theta)^2-2(z-\theta)\varepsilon+\varepsilon^2\right)\right)q(\varepsilon)d\varepsilon \\
&= \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{1}{2\sigma_1^2}(z-\theta)^2\right) \int \exp\left(-\frac{1}{2\sigma_1^2}\left(-2(z-\theta)\varepsilon+\varepsilon^2\right)\right)q(\varepsilon)d\varepsilon \\
&= \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{1}{2\sigma_1^2}(z-\theta)^2\right) \int \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2\sigma_1^2}\left(-2(z-\theta)\varepsilon+\varepsilon^2\right)-\frac{1}{2}\varepsilon^2\right)d\varepsilon
\end{align*}
Posterior exact when ?

\item
If $h_\theta$ monotonic, invertible:
\begin{align*}
z &= h_\theta(u;\bfeps) \\
q_\theta(z|\bfeps) &= q_u\left(h_\theta^{-1}(z;\bfeps)\right)\left|\nabla_zh_\theta^{-1}(z;\bfeps)\right| \\
q_\theta(z) &= \int q_u\left(h_\theta^{-1}(z;\bfeps)\right)\left|\nabla_zh_\theta^{-1}(z;\bfeps)\right| q(\bfeps)d\bfeps
\end{align*}
\todo: normalizing flow literature? Restrict $h_\theta$ to be independent of $\bfeps$ (e.g., linear flows)?

\end{itemize}


\newpage


\section{Semi-implicit variational inference}

Based on \citet{Yin:2018}.
\\

\sivi is addresses the issues of classical VI attributed to the requirement of a conditionally conjugate variational family by relaxing this requirement to allow for implicit distributional families from which samples can be drawn. This implicit family consists of hierarchical distributions with a mixing parameter. While the distribution conditioned on the mixing parameter is required to be analytical and reparameterizable, the mixing distribution can be arbitrarily complex. The use of such a variational family also addresses the problems of conventional mean-field families as dependencies between the latent variables can be introduced through the mixing distribution.
\\

The objective in \sivi is a surrogate \elbo that is only exact asymptotically and otherwise a lower bound of the \elbo~\citep{Molchanov:2019}. Like in black box VI, the gradients are rewritten as expectations and estimated via Monte Carlo samples.
\\

\citet{Molchanov:2019} extends \sivi to doubly \sivi for variational inference and variational learning in which both the variational posterior and the prior are semi-implicit distributions. They also show that the \sivi objective is a lower bound of the \elbo.
\\

\citet{Molchanova:2019} and \citet{Moens:2021} comment that \sivi and \uivi struggle in high-dimensional regimes. \mcmc methods also have high variance \citep{Moens:2021}.
\\

\citet{Moens:2021} introduce compositional implicit variational inference (CI-VI), which rewrites the \sivi \elbo as a compositional nested form $\E_\nu\left[f_\nu\left(\E_\omega\left[g_\omega(\theta)\right]\right)\right]$. The gradient involves estimated the nested expectations, for which a simple Monte-Carlo estimator would be biased. CI-VI uses an extrapolation-smoothing scheme for which the bias converges to zero with iterations. In practice, the gradient involves matrix-vector products that are expensive but can be approximated via sketching techniques. Under certain assumptions, convergence of the CI-VI algorithm is proved in terms of the number of oracle calls needed to convergence (\todo).


\section{Hierarchical variational inference}

Based on \citet{Ranganath:2016}.
\\

Predating \sivi and \uivi, \hvm first(?) addressed the restricted variational family issue of classical VI by using a hierarchical variational distribution which is enabled by \bbvi. \hvm considers a mean-field variational likelihood and a variational prior that is differentiable (e.g., a mixture or a normalizing flow). \hvm also optimizes a lower bound of the \elbo that is constructed using a recursive variational distribution that approximates the variational prior.


\newpage


\section{Theoretical guarantees for implicit VI}

\citet{Plummer:2021}.
\\

\todo: Considers non-linear latent variable model
\begin{align*}
z &= \mu(\varepsilon) + u \\
u &\sim N(0,\sigma^2) \\
\varepsilon &\sim U(0,1) \\
\mu &\sim \Pi_\mu \\
\sigma &\sim \Pi_\sigma
\end{align*}
where $\Pi_\mu$ and $\Pi_\sigma$ are priors. $\Pi_\mu$ is taken as Gaussian process?

For simple normal-normal model, KL divergence for true normal model and true posterior converges weakly to a $\chi_1^2$ and not to 0.


\newpage


\section{Other references}

VI review:
\begin{itemize}
\item
\href{https://s3.amazonaws.com/ieeecs.cdn.csdl.content/trans/tp/2019/08/08588399.pdf?AWSAccessKeyId=ASIA2Z6GPE73ITPNF4HV&Expires=1646432232&Signature=%2BwbeAhSyM%2FMqzaAhLaunhuq30BI%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEP7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIGoE%2FVHY77X2Kpq0J47Ic29AHEKJ%2F%2FS0foSv98aaP%2F%2BxAiAYu9FImOV6wjyu7v9HkfGlfyytEvSd2wujoh7wIivaTyqdAghmEAAaDDc0MjkwODcwMDY2MiIMYEqAQr9DL2OWCGlbKvoB58nRjaBdUqwysmhAcnlM2JP60usEfHlgP0Bn2xGYd1kZJPiEKzmA%2F76Bo7YRZ2z%2FpCEaHPV7aFI6ikqSiJToz5QlGqPUQp5GF6qR5TSKEJ5oMTODfVyUt5xdcYOww012wn%2FyqAE3T9hRTF%2FyxhZeY6Q1xrj3Bzu8p0jP7NO30Xoje2Oct8JA6sOwhgFEL5rjtIpPoiiAywnSeWz8Ia8ra4tpCOPbG8T3SLq%2BFE4SS3BrviPuGWH0b0vr2EzBSZEoWFySA87d27hh99%2B%2BsLKLvcu4u3VGRUWb%2FpwIYAT88MBxL1RhCazyRj4eByB%2FsbVAhmGQ0bYAYYZT1TDqhYqRBjqbAcZHTiSLr48H4Ha2O6bVtVfH9%2B8e%2FmWAg2joY8DUAAvhdiGeqGLdPfgpNE0%2BpXfJI%2BKR8LeVrJvtfpYI3LV2RQn%2B3y2cnzIrosiNxE68tL4U%2BsW4uor4O6eZ8qvF%2Bvr7ncAy4rulMRlaqpgY%2FehIGk5UxsUPTr4zC%2BYNJXlKu28DlNal14YG5ugTN3VC5sgxL%2B%2Bq%2BEp8ibbwRzSf}{Advances in Variational Inference} (2019)
\item
\href{https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773}{Variational Inference: A Review for Statisticians} (2017)
\item
\href{https://arxiv.org/pdf/1401.0118.pdf}{Black Box Variational Inference} (2013): dominated convergence theorem used to push gradient into expectation
\end{itemize}
Possibly related VI approaches/of interest
\begin{itemize}
\item
\href{https://proceedings.mlr.press/v80/yin18b/yin18b.pdf}{Semi-Implicit Variational Inference} (2018)

\href{https://proceedings.mlr.press/v89/molchanov19a/molchanov19a.pdf}{Doubly Semi-Implicit Variational Inference} (2019)

\href{https://openreview.net/pdf?id=HkxStk34Kr}{Structured Semi-Implicit Variational Inference} (2019): mentions that previous methods scale exponentially with dimension of the latent variables. Imposes that the high-dimensional semi-implicit distribution factorizes into a product of low-dimensional conditional semi-implicit distributions and shows that the resulting entropy bound is tighter than that of \sivi's and consequently a tighter \elbo objective.

\href{https://arxiv.org/pdf/2101.06070.pdf}{Efficient Semi-Implicit Variational Inference
} (2021)
\item
\href{https://arxiv.org/pdf/1702.08235.pdf}{Variational Inference using Implicit Distributions} (2017): implicit with density ratio estimation?
\item
\href{https://proceedings.neurips.cc/paper/2019/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf}{Importance Weighted Hierarchical
Variational Inference} (2019)
\item
\href{https://jmlr.org/papers/volume22/19-1028/19-1028.pdf}{Normalizing Flows for Probabilistic Modeling and Inference} (2021)

\href{https://arxiv.org/pdf/2002.09547.pdf}{Stochastic Normalizing Flows} (2020)
\end{itemize}
Theory/analysis
\begin{itemize}
\item
\href{https://proceedings.mlr.press/v130/plummer21a/plummer21a.pdf}{Statistical Guarantees for Transformation Based Models with Applications to Implicit Variational Inference} (2021)

\href{https://oaktrust.library.tamu.edu/bitstream/handle/1969.1/195122/PLUMMER-DISSERTATION-2021.pdf?sequence=1&isAllowed=y}{Statistical and Computational Properties of Variational Inference} (2021; thesis)
\item
\href{https://knowledge.uchicago.edu/record/2221?ln=en}{Theoretical Guarantees of Variational Inference and Its Applications} (2020; thesis)
\item
\href{https://tel.archives-ouvertes.fr/tel-02893465/document}{Contributions to the theoretical study of variational inference and robustness} (2020; thesis)
\item
\href{https://par.nsf.gov/servlets/purl/10180931}{On Statistical Optimality of Variational Bayes} (2018)

\href{https://math.unm.edu/~skripka/workshop_high_dim/Anirban_Talks.pdf}{Statistical guarantees for variational Bayes} (2021; slides)
\item
\href{https://arxiv.org/pdf/2010.09540.pdf}{Statistical Guarantees and Algorithmic Convergence Issues of Variational Boosting} (2020)
\item
\href{https://proceedings.neurips.cc/paper/2020/file/7cac11e2f46ed46c339ec3d569853759-Paper.pdf}{Robust, Accurate Stochastic Optimization for Variational Inference} (2020) -- iterates as \mcmc?
\item
\href{https://arxiv.org/pdf/1908.04847.pdf}{Convergence Rates of Variational Inference in Sparse Deep Learning} (2019)

\href{https://arxiv.org/pdf/1902.05068.pdf}{On the Convergence of Extended Variational Inference for Non-Gaussian Statistical Models} (2020)

\end{itemize}


\newpage

\bibliographystyle{plainnat}
\bibliography{../report/qp}

\end{document}