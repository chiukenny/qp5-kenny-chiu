\documentclass[10pt]{article}
\input{header}
\input{defs}

\title{\todo}
\author{Kenny Chiu}
\date{\today}

\begin{document}

\maketitle


\newpage


\section{Critical analysis}

\subsection{Introduction}

\citet{Titsias:2019} introduce \textit{unbiased implicit variational inference} (\uivi) as a variational inference method with a flexible variational family and that addresses the issues of the existing methods that it is built on. In this analysis, we summarize the work of \citet{Titsias:2019} in the context of the literature and discuss the strengths and limitations of \uivi. This analysis is organized as follows: Section~\ref{an:literature} introduces the problem context and previous work; Section~\ref{an:uivi} describes how \uivi works, how it addresses the limitations of previous methods, and its own limitations; and Section~\ref{an:postpaper} highlights related work in the recent literature and discusses the general direction that the literature is moving towards.

\subsection{Context and previous work} \label{an:literature}

Variational inference (\vi) \citep{Jordan:1999} is a Bayesian inference method that formulates the problem of finding the posterior distribution $p(\bfz|\bfx)$ of latent variables $\bfz$ given data $\bfx$ as an optimization problem. \vi posits a variational family $\calQ=\{q_\theta\}$ of distributions indexed by variational parameters $\theta$, and the goal is to identify the variational distribution $q_\theta(\bfz)\in\calQ$ that best approximates the posterior distribution. In standard \vi, the selected distribution $q_\theta$ is the one that minimizes the Kullback-Leibler (\kl) divergence of $q_\theta$ and $p(\bfz|\bfx)$, or equivalently, the one that maximizes the evidence lower bound (\elbo) denoted as
\[
\calL(\theta) = \E_{q_\theta(\bfz)}\left[\log p(\bfx,\bfz) - \log q_\theta(\bfz)\right] \;.
\]
Standard \vi maximizes the \elbo using a coordinate ascent algorithm, which requires placing strong restrictions on the choice of the model and the variational family. These restrictions include (1) a mean-field assumption where the latent variables $\bfz$ are marginally independent and the variational distribution factorizes as $q_\theta(\bfz)=\prod_{i=1}^dq_{\theta_i}(\bfz_i)$, and (2) the model has conjugate conditionals where $p(\bfz_i)$ and $p(\bfz_i|\bfx,\bfz_{\neg i})$ are from the same distribution family. These assumptions implied that the choice of a variational family were often limited to analytical, exponential families and that marginal dependencies could not be modeled.
\\

An important development after standard \vi was black box \vi (\bbvi) \citep{Ranganath:2014}, which relaxed the restrictive assumptions by optimizing the \elbo using a different approach. By rewriting the \elbo gradient in terms of an expectation, the gradient could be estimated unbiasedly and cheaply using Monte Carlo samples. The optimization approach of \bbvi exchanges the restrictive assumptions of standard \vi for the different assumption that one can sample from the variational distribution $q_\theta(\bfz)$. This expanded the possibilities for the choice of the variational family. One such proposed family was the hierarchical variational model (\hvm) \citep{Ranganath:2016} containing distributions of the form $q_\theta(\bfz) = \int q_\theta(\bfz|\bfeps)q_\theta(\bfeps)d\bfeps$. An advantage of these hierarchical distributions over other variational distributions is the ease in being able to capture marginal dependencies between latent variables through the mixing distribution $q_\theta(\bfeps)$.
\\

Further pushing the assumption that one only needs to be able to sample from the variational distribution, one trend following the introduction of the \hvm was the incorporation of deep neural networks to expand the modeling capacity of the hierarchical variational family. These models took various forms, such as through normalizing flows~\citep{Rezende:2015} or through implicit distributions~\citep{Mohamed:2016} involving deep networks in which the density cannot be evaluated. Though the implicit models are flexible, the log density ratio in the \elbo is intractable in these models. Some works proposed using density ratio estimation to tackle this problem~\citep[e.g.,][]{Mohamed:2016,Huszar:2017}, but this approach is known to struggle in high-dimensional regimes~\citep{Sugiyama:2012}.
\\

The method that precedes \uivi and that was proposed to address the challenges of using implicit distributions in hierarchical variational models is \textit{semi-implicit} \vi (\sivi)~\citep{Yin:2018}. \textsc{Sivi} makes use of a semi-implicit variational distribution in which the variational conditional $q_\theta(\bfz|\bfeps)=q(\bfz|\bfeps)$ is required to be reparameterizable~\citep{Kingma:2013} and explicit while the mixing distribution $q_\theta(\bfeps)$ is required to be also reparameterizable but possibly implicit. \textsc{Sivi} then avoids the density ratio estimation problem by instead optimizing a lower bound for the \elbo that is only exact as the number of samples in each iteration goes to infinity~\citep{Yin:2018,Molchanov:2019}.


\subsection{Current work} \label{an:uivi}

\citet{Titsias:2019} propose \uivi as an alternative to \sivi that directly maximizes the \elbo as an objective rather than a surrogate lower bound. The motivation for \uivi is that directly optimizing the \elbo objective should be more efficient than optimizing a surrogate and therefore should result in faster convergence to the optimal variational approximation. \textsc{Uivi} allows for an \elbo objective by rewriting the \elbo gradient in terms of two expectations. One expectation is easily estimated using Monte Carlo samples, while the other expectation is over an inverse conditional from which \uivi draws samples using Markov chain Monte Carlo (\mcmc).

\subsubsection{Unbiased implicit variational inference}

Like in \sivi, \uivi starts with a hierarchical variational model setup where the variational distribution is
\[
q_\theta(\bfz) = \int q_\theta(\bfz|\bfeps)q(\bfeps)d\bfeps \;.
\]
\textsc{Uivi} requires the variational conditional $q_\theta(\bfz|\bfeps)$ to be reparameterizable, i.e., that any sample $\bfz\sim q_\theta(\bfz|\bfeps)$ can be rewritten as
\[
\bfz = h_\theta(\bfu;\bfeps) := h_{\bfpsi=g_\theta(\bfeps)}(\bfu) 
\]
where $h_{\bfpsi}$ is some reparameterization function with parameters $\bfpsi$ that are the output of some arbitrarily complex function $g_\theta$ that depends on variational parameters $\theta$ and input $\bfeps$. To sample from $q_\theta(\bfz)$, noise variables $\bfu\sim q(\bfu)$ and $\bfeps\sim q(\bfeps)$ are first sampled from fixed auxiliary distributions and then fed through $h_\theta$. \textsc{Uivi} also requires that $q_\theta(\bfz|\bfeps)$ and its log-gradient $\nabla_\bfz\log q_\theta(\bfz|\bfeps)$ can be evaluated, which holds for common reparameterizable distributions such as Gaussian.
\\

Under these assumptions, the \elbo can be rewritten as an expectation with respect to the noise distributions $q(\bfu)$ and $q(\bfeps)$ through a change of variables, and its gradient can be decomposed into two expectation terms given by
\[
\nabla_\theta \calL(\theta) = \E_{q(\bfeps)q(\bfu)}\left[\nabla_\bfz \log p(\bfx,\bfz)\big|_{\bfz=h_\theta(\bfu;\bfeps)}\nabla_\theta h_\theta(\bfu;\bfeps)\right] - \E_{q(\bfeps)q(\bfu)}\left[\nabla_\bfz \log q(\bfz)\big|_{\bfz=h_\theta(\bfu;\bfeps)}\nabla_\theta h_\theta(\bfu;\bfeps)\right] \;.
\]
The first expectation can be estimated using samples from $q(\bfeps)$ and $q(\bfu)$ while the second expectation is more difficult as $\nabla_z\log q(\bfz)$ may not be computable if $q(\bfz)$ is implicit. The first key trick in \uivi is to rewrite the gradient in the second term as an expectation given by
\[
\nabla_z\log q(\bfz) = \E_{q_\theta(\bfeps|\bfz)}\left[\nabla_\bfz\log q_\theta(\bfz|\bfeps)\right]
\]
which then allows for Monte Carlo estimation using samples from $q_\theta(\bfeps|\bfz)\propto q_\theta(\bfz|\bfeps)q(\bfeps)$. A \mcmc sampler is used to sample from $q_\theta(\bfeps|\bfz)$, and the second key trick in \uivi is to reuse the sample $(\bfz_i,\bfeps_i)$ used to estimate the outer expectation as an initial point in the \mcmc sampler. As the initial point is a sample from the same joint distribution $q_\theta(\bfz,\bfeps)$, no burn-in is necessary and the only purpose of the \mcmc is to break the dependence between samples used to estimate the inner and outer expectations. Thus, the gradient of the \elbo is estimated by
\[
\widehat{\nabla}_\theta\calL(\theta) = \frac{1}{n}\sum_{i=1}^n \left(\nabla_\bfz\log p(\bfx,\bfz)\big|_{\bfz=h_\theta(\bfu_i;\bfeps_i)} - \frac{1}{m}\sum_{j=1}^m \nabla_\bfz\log q_\theta(\bfz|\bfeps_j')\big|_{\bfz=h_\theta(\bfu_i;\bfeps_i)} \right)\nabla_\theta h_\theta(\bfu_i;\bfeps_i)
\]
where $\bfeps_i\sim q(\bfeps)$, $\bfu_i\sim q(\bfu)$, $\bfeps_j'\sim q_\theta(\bfeps|\bfz)$ and with $n=1$, $m=5$ said to be used in practice.

\subsubsection{Other contributions}

Aside from the \uivi algorithm, other contributions of the paper by \citet{Titsias:2019} include the empirical evaluations of \uivi on synthetic and benchmark datasets. Using a Gaussian conditional with a neural network for the mean parameter, Hamiltonian Monte Carlo (\hmc) for the \mcmc estimation of the \elbo gradient, and otherwise a fairly standard setup, \uivi is shown to be able to visually approximate various synthetic 2D distributions. Under a similar setup, \uivi is shown to be able to achieve better predictive performance than \sivi on the MNIST and HAPT~\citep{Reyes:2014} datasets while being comparable in terms of time per iteration. Finally, \citet{Titsias:2019} show that for a variational autoencoder (\vae)~\citep{Kingma:2013} with a semi-implicit variational distribution, \uivi achieves a greater marginal log-likelihood on the test set compared to standard \vae and \sivi on the MNIST and Fashion-MNIST datasets.

\subsubsection{Limitations}

The paper by \citet{Titsias:2019} has a few limitations. The main limitation is the lack of theoretical guarantees for the performance and convergence of \uivi. For example, it is unclear what the modeling capabilities are for the \uivi variational family, and no guidance is given on how to construct the model such that the true posterior is in or at least well-approximated by a member of the variational family. It is also unclear how good of an approximation \uivi is able to guarantee through its optimization procedure. However, we recognize that this is a common problem across the \vi literature and generally stems from the challenge of analyzing general purpose methods that may include intractable and non-analytic components.
\\

Another notable limitation of the paper is the missing discussion of the limitations of \uivi. In particular, the showcased experiments do not stress test \uivi, and there is no mention of future directions for improving or extending \uivi. Related work published after the paper by \citet{Titsias:2019} reported limited scalability with the number of latent parameters~\citep{Molchanova:2019,Moens:2021}. This is likely a consequence of the stochastic optimization of the \elbo as well as the use of \mcmc, both for which require an increasing number of samples to maintain estimation quality in response to an increasing number of dimensions. The \mcmc sampling in the \uivi optimization procedure may also result in greater variance of the \elbo gradient estimates~\citep{Betancourt:2015} and further contribute to non-scalability by complicating potential parallelization of the algorithm~\citep{Sobolev:2019}. In some instances, other issues common to \mcmc approaches, such as poor mixing over different modes, appear to inhibit the performance of \uivi~\citep{Sobolev:2019}.


\subsection{Other related work} \label{an:postpaper}

While \uivi was proposed as an improved alternative to \sivi, there does not appear to be follow-up work in the literature that directly extends \uivi. As mentioned in the previous section, the inefficiency of \mcmc in high-dimensional regimes is often cited as the main problem of \uivi~\citep{Molchanova:2019,Moens:2021}. It appears that rather than trying to address this issue in \uivi, recent work in the literature return to \sivi and propose methods that either improve the quality of its approximation or allow it to scale more efficiently to high dimensions.
\\

Several strategies for improving the \sivi approximation have been proposed in the literature around the time of or after the work by \citet{Titsias:2019}. \citet{Molchanov:2019} proposed \textit{doubly}~\sivi (\dsivi) that expands the flexibility of standard \sivi by allowing both the posterior and prior to be semi-implicit. \citet{Sobolev:2019} introduced \textit{importance weighted hierarchical}~\vi (\iwhvi), which optimizes a \sivi-like lower bound that incorporates elements from the bound used in importance weighted autoencoders~\citep{Burda:2015}. \textsc{Sivi}, \dsivi and \hvm can be seen as special cases of \iwhvi and so the bound in \iwhvi has the capacity to result in a tighter lower bound~\citep{Sobolev:2019}.
\\

Recent work in the literature have focused more on improving the scalability of \sivi to high dimensions. \citet{Molchanova:2019} proposed \textit{structured}~\sivi where the high-dimensional semi-implicit distribution is assumed to factorize into low-dimensional semi-implicit distributions. \citet{Moens:2021} introduced \textit{compositional implicit}~\vi, which integrates various mechanisms into \sivi including an adaptive solver for addressing the bias in the \sivi objective and sketch-based approximations that keep the method computationally practical for high-dimensional regimes.
\\

Though the developments in the related literature are mostly methodological, there have been some recent forays into the more theoretical side that attempt to provide statistical guarantees and insights for implicit \vi. In particular, \citet{Plummer:2021} derive posterior contraction results for simple \textit{non-linear latent variable models} by drawing connections to Gaussian convolutions. The \nllvm has a structure that can be seen as a particular choice of the reparameterization and mixing distributions in \uivi, and so we suspect that \citeauthor{Plummer:2021}'s work may provide a reasonable starting point for a theoretical analysis of \uivi.


\newpage


\section{Project report}

\todo title

\vspace{2em}
\begin{abstract}
\todo
\end{abstract}
\vspace{2em}

\subsection{Introduction}

\todo

To approximate $p(z|x)$, \uivi posits the variational family $\calQ$ of distributions of the form
\[
q_\theta(z) = \int q_\theta(z|\bfeps)q(\bfeps)\lambda(d\bfeps) \;.
\]
where the variational conditional $q_\theta(z|\bfeps)$ is reparameterizable and explicit, but the dependency on $\theta$ can be arbitrarily complex. \textsc{Uivi} also requires that the log-gradient $\nabla_z\log q_\theta(z|\bfeps)$ can be evaluated.


\subsection{Notation}

\todo $\phi_\sigma$ density of $\calN(0,\sigma^2\bfI_d)$. Overload distribution and density. $\lambda$ Lebesgue measure on $[0,1]$. Borel $\sigma$-algebra of $\bbR$ by $\calB$. 

\todo delete $C^\beta(\calZ)$ $\beta$-H\"{o}lder. $\|\argdot\|_\infty$ supremum norm

\subsection{Quality of approximation}

\todo overly simple example showing true posterior not in variational family?

\citet{Titsias:2019} empirically show that \uivi is seemingly able to match the implicit variational distribution to various synthetic datasets and is able to better approximate several models compared to \sivi. However, they do not provide theoretical guarantees nor quantify the quality of the \uivi approximation. In this section, we show that \todo simple example where the true posterior distribution is not in variational family, and that under certain assumptions and choices of the reparameterization and mixing distribution, \uivi is able to approximate the true posterior arbitrarily closely. We do so using similar arguments that \citet{Plummer:2021} made for non-linear latent variable models (\nllvm). Following their work, we assume that $z$ is a continuous, univariate latent variable. We leave other cases for future work due to time constraints on this project.

\subsubsection{Normal model example}

We first illustrate through a simple example that using a hierarchical variational family does not magically expand the modeling capacity of the variational distribution, and that the choice of the reparameterization and mixing distributions are still important in realizing the potential of \uivi.
\\

Suppose that the posterior $p(z|x)$ is univariate Gaussian with mean $\mu_{Z|X}$ and known variance $\sigma_{Z|X}^2$. This may be the case, for example, when we have a Gaussian likelihood with latent mean $Z$ and known variance, and a conjugate Gaussian prior for $Z$. Suppose that we choose to approximate $p(z|x)$ by the \uivi variational family with
\[
z = h_\theta(u;\bfeps) = \theta + \bfeps + \sigma_{Z|X}u
\]
where $\bfeps$ and $u$ are independent standard normal random variables and $\theta$ is the variational parameter to optimize. Then it is easy to see that
\begin{align*}
q_\theta(z) &= \int_\bbR q_\theta(z|\bfeps)q(\bfeps)\lambda(d\bfeps) \\
&= \int_\bbR \phi_{\sigma_{Z|X}}(\theta+\bfeps)\phi_1(\bfeps) \lambda(d\bfeps) \\
&= \phi_{\sqrt{\sigma_{Z|X}^2+1}}(\theta) \;.
\end{align*}
In other words, our variational family is the set of univariate Gaussian distributions $\left\{\calN(\theta,\sigma_{Z|X}^2+1):\theta\in\bbR\right\}$. The true posterior distribution $\calN(\mu_{Z|X},\sigma_{Z|X}^2)$ is not in this variational family. The problem in this example is that our reparameterized distribution is too restrictive and misspecified. If we changed the reparameterization function to be
\[
z = h_{\bm{\theta}}'(u;\bfeps) = \theta_1 + \bfeps + \theta_2u
\]
where both $\theta_1$ and $\theta_2$ are learned parameters, then the new variational family corresponding to this reparameterization includes the true posterior distribution. While this example is very simple, it nicely illustrates that relying on the hierarchical structure alone is insufficient for setting up a flexible variational family.


\subsubsection{Flexible variational family}

Consider the \uivi variational family induced by the following choices of the reparameterization and mixing distribution. We take a potentially multivariate mixing distribution of the form $q(\bfeps)=\prod_{i=1}^dq(\bfeps_i)$ where $q(\bfeps_i)=\mathrm{Unif}(0,1)$ for $i=1,\ldots,d$, $d\geq 1$. Let the variational conditional $q_{\theta,\sigma}(z|\bfeps)$ be univariate Gaussian with mean $\mu_\theta(\bfeps)$ and variance $\sigma^2$ where $\mu_\theta:[0,1]^d\rightarrow\bbR$ is some arbitrarily complex function. Note that we keep the variational parameters $\theta$ and $\sigma$ separate for reasons that will be clear shortly. This distribution is reparameterizable through the form
\[
z = h_{\theta,\sigma}(u;\bfeps) = \mu_\theta(\bfeps) + \sigma u
\]
where $u\sim \calN(0,1)$. Furthermore, its log-density and its gradient are given by
\begin{align*}
\log q_{\theta,\sigma}(z|\bfeps) &= -\frac{1}{2}\log\left(2\pi\sigma^2\right)-\frac{1}{2\sigma^2}\left(z-\mu_\theta(\bfeps)\right)^2 \;, \\
\nabla_z \log q_{\theta,\sigma}(z|\bfeps) &= -\frac{1}{\sigma^2}\left(z-\mu_\theta(\bfeps)\right) \;.
\end{align*}
The above properties suggest that the variational family induced by these choices satisfy the \uivi requirements. Note that the form of $h_{\theta,\sigma}$ also resembles the \nllvm studied by \citet{Plummer:2021}, which allows us to apply their results with slight modifications.
\\

To study the approximation capability of this model, the key insight of \citet{Plummer:2021} is that $q_{\theta,\sigma}(z)$ has the form of a convolution with a Gaussian kernel, that is,
\begin{align*}
q_{\theta,\sigma}(z) &= \int_0^1 q_{\theta,\sigma}(z|\bfeps)q(\bfeps)\lambda(d\bfeps) \\
&= \int_0^1 \phi_\sigma\left(z-\mu_\theta(\bfeps)\right)\lambda(d\bfeps) \\
&= \int_\bbR \phi_\sigma\left(z-t\right)\nu_{\mu_\theta}(dt)
\end{align*}
where $\nu_{\mu_\theta}(B)=\lambda\left(\mu_\theta^{-1}(B)\right)$, $B\in\calB$, is the image measure of $\lambda$ under $\mu_\theta$. Using the approximation property of Gaussian convolutions, we characterize the relationship between the true posterior and the variational family through the following proposition.

\begin{proposition} \label{prop:membership}
Let $\calQ_\sigma$ denote the variational family described above indexed by the standard deviation $\sigma$ of $q_{\theta,\sigma}(z|\bfeps)$. Suppose that $\mu_\theta(t)=F_{z|x}^{-1}(t)$ for all $t\in[0,1]$. Then $p(z|x)\in\calQ_0$.
\end{proposition}
\begin{proof}
If $\mu_\theta(t)=F_{z|x}^{-1}(t)$ for all $t\in[0,1]$, then $q_{\theta,\sigma}(z) = \phi_\sigma*p(z|x)$. The result immediately follows using the property of Gaussian convolutions that $\phi_\sigma*p(z|x)\rightarrow p(z|x)$ pointwise as $\sigma\rightarrow0$.
\end{proof}

Proposition~\ref{prop:membership} says that if the inverse CDF or quantile function $F_{z|x}^{-1}$ is in the set of functions $\{\mu_\theta:\theta\in\Theta\}$ that can be modeled by $\mu_\theta$, then the true posterior $p(z|x)$ is a limiting member of the sequence of best approximations by this variational family as the bandwidth $\sigma$ of the Gaussian kernel shrinks to zero. While this result suggests that the true posterior is not in the variation family for $\sigma>0$, it does imply that for any measure of error, we can choose $\sigma$ such that the best approximation will be close to the true posterior within a desired tolerance level. What this result does not address is whether we are able to achieve the best approximation for a given $\sigma$ in practice. This depends on whether our functional form $\mu_\theta$ is flexible enough such that $F_{z|x}^{-1}\in\{\mu_\theta:\theta\in\Theta\}$ and whether our optimization procedure is able to identify the correct $\theta$ such that $\mu_\theta=F_{z|x}^{-1}$.
\\

\todo: problem boils down to learning the quantile function. 

\subsubsection{\todo to delete}

\todo the results below seem to be leading towards posterior contraction; proposition 2 isn't a result about $q_{\theta,\sigma}(z)$. Theorem 3.1 may still be relevant?

We can further quantify the quality of the approximation by $q_{\theta,\sigma}(z)$ if we make assumptions about the smoothness of $p(z|x)$ and its support. Following \citet{Plummer:2021}, we make the following assumptions.

\begin{assumption} \label{asp:p1}
$\log p(z|x) \in C^\beta([0,1])$. Define $l_j(z_0) = \nabla_z^j\log p(z|x)\big|_{z=z_0}$ for $j=1,\ldots,r$ with $r=\lfloor\beta\rfloor$. For any $\beta>0$, there exists a constant $L>0$ such that for all $z_1\neq z_2$,
\[
|l_r(z_1)-l_r(z_2)| \leq L|z_1-z_2|^{\beta-r} \;.
\]
\end{assumption}

\begin{assumption} \label{asp:p2}
$p(z|x)$ has compact support on $[0,1]$. There exists some interval $[z_1,z_2]\subset[0,1]$ such that $p(z|x)$ is non-decreasing on $[0,z_1]$, non-zero on $[z_1,z_2]$, and non-increasing on $[z_2,1]$.
\end{assumption}

Assumption~\ref{asp:p1} says that the derivatives of $\log p(z|x)$ up to order $r$ are $\beta$-H\"{o}lder continuous, implying that $\log p(z|x)$ is smooth to an extent. The proofs of \citet{Kruijer:2010} and \citet{Plummer:2021} rely heavily on the assumed smoothness in order to ensure that the error between the target distribution and an approximating convolution can be bounded. Assumption~\ref{asp:p2} says that the mass of $p(z|x)$ is concentrated in some compact interval of $z$ and that the tails of $p(z|x)$ outside this interval can be bounded above. This allows the approximation error in the tails to be bounded even as the convolution bandwidth shrinks, and so an analysis of the error only needs to focus on the closed interval in which the mass is concentrated. \citet{Plummer:2021} appear to specify an interval of $[0,1]$ for analytical convenience, whereas the intervals in similar assumptions made by \citet{Ghosal:1999} feature arbitrary finite endpoints.
\\

Under Assumptions~\ref{asp:p1} and \ref{asp:p2}, \citet{Plummer:2021} follow the work of \citet{Kruijer:2010} and consider a sequence of functions $\{p_j\}_{j\geq0}$ constructed through an iterative procedure given by
\begin{align*}
p_{j+1}(z|x) &= p(z|x) - \Delta_\sigma p_j(z|x) \;, \\
\Delta_\sigma p_j(z|x) &= \phi_\sigma * p_j(z|x) - p_j(z|x)
\end{align*}
with $p_0(z|x)=p(z|x)$. The quality of the approximation is then characterized in terms of the error between the convolution \todo

\begin{proposition}
Suppose that $p(z|x)$ satisfies Assumptions~\ref{asp:p1} and \ref{asp:p2} with $\beta\in(2j,2j+2]$. Let $F_{z|x}$ be the cumulative distribution function of the posterior $p(z|x)$. \todo fix this If $\mu_\theta(t)=F_{z|x}^{-1}(t)$ for all $t\in[0,1]$, then
\[
\|\phi_\sigma * p_\beta(z|x) - p(z|x)\|_\infty = O(\sigma^\beta)
\]
with
\[
\phi_\sigma * p_\beta(z|x) = p(z|x)\left(1+O(\sigma^\beta)\left(\sum_{i=1}^rc_i|l_j(z)|^{\frac{\beta}{i}}+c_{r+1}\right)\right)
\]
for non-negative constants $c_i$, $i=1,\ldots,r+1$ and $z\in[0,1]$.
\end{proposition}
\begin{proof}
Suppose that $\mu_\theta(\bft)=F_{\bfz|\bfx}^{-1}(\bft)$ and so $q_\theta(\bfz)=\phi_\sigma*p(\bfz|\bfx)$. Then as $\sigma\rightarrow0$, $q_\theta(\bfz)\rightarrow p(\bfz|\bfx)$.
\end{proof}


\subsection{Variance of gradient}

\todo

label switching with mixtures?


\subsection{Discussion}


\newpage


\bibliographystyle{plainnat}
\bibliography{qp}

\end{document}