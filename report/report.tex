\documentclass[10pt]{article}
\input{header}
\input{defs}

\title{\todo}
\author{Kenny Chiu}
\date{\today}

\begin{document}

\maketitle


\newpage


\section{Critical analysis}

\subsection{Introduction}

\todo \citet{Titsias:2019} introduce Unbiased Implicit Variational Inference (\uivi) as a variational inference method that allows for a flexible variational family and that addresses the issues of the methods that it is built on. In this analysis, we summarize the work of \citet{Titsias:2019} in the context of the literature and critically examine the strengths and limitations of \uivi. This analysis is organized as follows: Section~\ref{an:literature} introduces the problem context and previous work; Section~\ref{an:uivi} describes how \uivi works and how it addresses the limitations of previous methods, and discusses its own limitations.

\subsection{Context and previous work} \label{an:literature}

Variational inference (\vi) \citep{Jordan:1999} is a Bayesian inference method that formulates the problem of finding the posterior distribution $p(\bfz|\bfx)$ of latent variables $\bfz$ given data $\bfx$ as an optimization problem. \vi posits a variational family $\calQ=\{q_\theta\}$ of distributions indexed by variational parameters $\theta$ and aims to approximate the posterior distribution by some simpler variational distribution $q_\theta(\bfz)\in\calQ$. In standard \vi, the selected distribution $q_\theta$ is the one that minimizes the Kullback-Leibler (\kl) divergence of $q_\theta$ and $p(\bfz|\bfx)$ or equivalently, the one that maximizes the evidence lower bound (\elbo) denoted as
\[
\calL(\theta) = \E_{q_\theta(\bfz)}\left[\log p(\bfx,\bfz) - \log q_\theta(\bfz)\right] \;.
\]
To maximize the \elbo, standard \vi places strong restrictions on the choice of the model and the variational family in order to allow the use of a coordinate ascent algorithm. These restrictions include (1) a mean-field assumption where the latent variables are marginally independent and the variational distribution factorizes as $q_\theta(\bfz)=\prod_{i=1}^dq_{\theta_i}(\bfz_i)$ and (2) the model has conjugate conditionals where $p(\bfz_i)$ and $p(\bfz_i|\bfx,\bfz_{\neg i})$ are from the same distribution family.
\\

A major development from standard \vi was black box \vi (\bbvi) \citep{Ranganath:2014} which relaxed the restrictive assumptions by optimizing the \elbo through a different approach. By rewriting the \elbo gradient in terms of an expectation, gradients could be estimated via Monte Carlo approaches. Exchanging the above assumptions for the different assumption that one can sample from the variational distribution $q_\theta(\bfz)$, this expanded the possibilities for the choice of the variational family. In particular, hierarchical variational families \citep{Ranganath:2016} of the form $q_\theta(\bfz) = \int q_\theta(\bfz|\bfeps)q_\theta(\bfeps)d\bfeps$ were proposed with their main strength being the ability to model marginal dependencies between latent variables (which the mean-field family could not) through the mixing distribution $q_\theta(\bfeps)$. 

\todo \sivi

\todo black box vi and hierarchical methods + limitations

\subsection{Current work} \label{an:uivi}

\todo how \uivi works, main advantages, other contributions of paper (empirical vs sivi), limitations


\newpage


\section{Project report}

\todo title

\vspace{2em}
\begin{abstract}
\todo
\end{abstract}
\vspace{2em}

\subsection{Introduction}


\newpage


\bibliographystyle{plainnat}
\bibliography{qp}

\end{document}